package io.delta.flink.source;

import java.util.List;

import io.delta.flink.source.builder.DeltaSourceBuilderSteps.BuildStep;
import io.delta.flink.source.builder.DeltaSourceBuilderSteps.HadoopConfigurationStep;
import io.delta.flink.source.builder.DeltaSourceBuilderSteps.TableColumnNameStep;
import io.delta.flink.source.builder.DeltaSourceBuilderSteps.TableColumnTypeStep;
import io.delta.flink.source.builder.DeltaSourceBuilderSteps.TablePathStep;
import io.delta.flink.source.internal.BaseDeltaSourceStepBuilder;
import io.delta.flink.source.internal.DeltaSourceConfiguration;
import io.delta.flink.source.internal.state.DeltaSourceSplit;
import org.apache.flink.core.fs.Path;
import org.apache.flink.formats.parquet.ParquetColumnarRowInputFormat;
import org.apache.flink.table.data.RowData;
import org.apache.flink.table.types.logical.LogicalType;
import org.apache.hadoop.conf.Configuration;

public final class RowDataDeltaSourceStepBuilder extends BaseDeltaSourceStepBuilder<RowData> {

    private RowDataDeltaSourceStepBuilder() {
        super();
    }

    /**
     * Creates an instance of Delta Source Step builder with exposed first build step {@link
     * TablePathStep}
     */
    public static TablePathStep<RowData> stepBuilder() {
        return new RowDataDeltaSourceStepBuilder();
    }

    /*
        We have to expose/override all methods from base class to include them in Javadoc
        generated by sbt-unidoc even though we're simply redirecting the call to super class.
    */
    @Override
    public TableColumnNameStep<RowData> tablePath(Path tablePath) {
        return super.tablePath(tablePath);
    }

    /**
     * Defines an array column names that should be read from Delta table.
     */
    @Override
    public TableColumnTypeStep<RowData> columnNames(String[] columnNames) {
        return super.columnNames(columnNames);
    }

    @Override
    public HadoopConfigurationStep<RowData> columnTypes(LogicalType[] columnTypes) {
        return super.columnTypes(columnTypes);
    }

    @Override
    public BuildStep<RowData> hadoopConfiguration(Configuration configuration) {
        return super.hadoopConfiguration(configuration);
    }

    @Override
    public BuildStep<RowData> versionAsOf(long snapshotVersion) {
        return super.versionAsOf(snapshotVersion);
    }

    @Override
    public BuildStep<RowData> timestampAsOf(long snapshotTimestamp) {
        return super.timestampAsOf(snapshotTimestamp);
    }

    @Override
    public BuildStep<RowData> startingVersion(String startingVersion) {
        return super.startingVersion(startingVersion);
    }

    @Override
    public BuildStep<RowData> startingTimestamp(String startingTimestamp) {
        return super.startingTimestamp(startingTimestamp);
    }

    @Override
    public BuildStep<RowData> updateCheckIntervalMillis(long updateCheckInterval) {
        return super.updateCheckIntervalMillis(updateCheckInterval);
    }

    @Override
    public BuildStep<RowData> ignoreDeletes(boolean ignoreDeletes) {
        return super.ignoreDeletes(ignoreDeletes);
    }

    @Override
    public BuildStep<RowData> ignoreChanges(boolean ignoreChanges) {
        return super.ignoreChanges(ignoreChanges);
    }

    @Override
    public BuildStep<RowData> option(String optionName, String optionValue) {
        return super.option(optionName, optionValue);
    }

    @Override
    public BuildStep<RowData> option(String optionName, boolean optionValue) {
        return super.option(optionName, optionValue);
    }

    @Override
    public BuildStep<RowData> option(String optionName, int optionValue) {
        return super.option(optionName, optionValue);
    }

    @Override
    public BuildStep<RowData> option(String optionName, long optionValue) {
        return super.option(optionName, optionValue);
    }

    @Override
    public BuildStep<RowData> partitions(List<String> partitions) {
        return super.partitions(partitions);
    }

    @Override
    public BuildStep<RowData> continuousMode() {
        return super.continuousMode();
    }

    /**
     * Builds an instance of {@link DeltaSource} that will produce elements of {@link RowData}
     * type.
     */
    @Override
    @SuppressWarnings("unchecked")
    public DeltaSource<RowData> build() {
        // TODO test this
        validateOptionExclusions();

        // TODO add option value validation. Check for null, empty values, numbers for
        //  "string" like values and string for numeric options.
        ParquetColumnarRowInputFormat<DeltaSourceSplit> format = buildFormat();

        return new DeltaSource<>(tablePath, format,
            (isContinuousMode())
                ? DEFAULT_CONTINUOUS_SPLIT_ENUMERATOR_PROVIDER
                : DEFAULT_BOUNDED_SPLIT_ENUMERATOR_PROVIDER,
            hadoopConfiguration, sourceConfiguration);
    }

    private ParquetColumnarRowInputFormat<DeltaSourceSplit> buildFormat() {
        ParquetColumnarRowInputFormat<DeltaSourceSplit> format;
        if (partitions == null || partitions.isEmpty()) {
            format = buildFormatWithoutPartitions(columnNames, columnTypes, hadoopConfiguration,
                sourceConfiguration);
        } else {
            // TODO PR 8
            throw new UnsupportedOperationException("Partition support will be added later.");
            /*format =
                buildPartitionedFormat(columnNames, columnTypes, configuration, partitions,
                    sourceConfiguration);*/
        }
        return format;
    }

    // TODO After PR 8
    private ParquetColumnarRowInputFormat<DeltaSourceSplit> buildPartitionedFormat(
        String[] columnNames, LogicalType[] columnTypes, Configuration configuration,
        List<String> partitionKeys, DeltaSourceConfiguration sourceConfiguration) {

        // TODO After PR 8
        /*return ParquetColumnarRowInputFormat.createPartitionedFormat(
            configuration,
            RowType.of(columnTypes, columnNames),
            partitionKeys, new DeltaPartitionFieldExtractor<>(),
            sourceConfiguration.getValue(PARQUET_BATCH_SIZE),
            PARQUET_UTC_TIMESTAMP,
            PARQUET_CASE_SENSITIVE);*/
        return null;
    }
}
