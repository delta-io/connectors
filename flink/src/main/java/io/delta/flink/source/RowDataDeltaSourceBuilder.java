package io.delta.flink.source;

import java.util.List;

import io.delta.flink.source.internal.BaseDeltaSourceBuilder;
import io.delta.flink.source.internal.DeltaSourceConfiguration;
import io.delta.flink.source.internal.state.DeltaSourceSplit;
import org.apache.flink.core.fs.Path;
import org.apache.flink.formats.parquet.ParquetColumnarRowInputFormat;
import org.apache.flink.table.data.RowData;
import org.apache.flink.table.types.logical.LogicalType;
import org.apache.hadoop.conf.Configuration;

public final class RowDataDeltaSourceBuilder
    extends BaseDeltaSourceBuilder<RowData, RowDataDeltaSourceBuilder> {

    private RowDataDeltaSourceBuilder(Path tablePath,
        String[] columnNames, LogicalType[] columnTypes,
        Configuration hadoopConfiguration) {
        super(tablePath, columnNames, columnTypes, hadoopConfiguration);
    }

    /**
     * Creates {@link RowDataDeltaSourceStepBuilder} for {@link DeltaSource} that produces elements
     * of type {@link RowData}
     */
    public static RowDataDeltaSourceBuilder builder(
        Path tablePath, String[] columnNames, LogicalType[] columnTypes,
        Configuration hadoopConfiguration) {
        return new RowDataDeltaSourceBuilder(
            tablePath, columnNames, columnTypes, hadoopConfiguration);
    }

    /*
        We have to expose/override all methods from base class to include them in Javadoc
        generated by sbt-unidoc even though we're simply redirecting the call to super class.
    */
    @Override
    public RowDataDeltaSourceBuilder versionAsOf(long snapshotVersion) {
        return super.versionAsOf(snapshotVersion);
    }

    @Override
    public RowDataDeltaSourceBuilder timestampAsOf(long snapshotTimestamp) {
        return super.timestampAsOf(snapshotTimestamp);
    }

    @Override
    public RowDataDeltaSourceBuilder startingVersion(long startingVersion) {
        return super.startingVersion(startingVersion);
    }

    @Override
    public RowDataDeltaSourceBuilder startingTimestamp(long startingTimestamp) {
        return super.startingTimestamp(startingTimestamp);
    }

    @Override
    public RowDataDeltaSourceBuilder updateCheckIntervalMillis(long updateCheckInterval) {
        return super.updateCheckIntervalMillis(updateCheckInterval);
    }

    @Override
    public RowDataDeltaSourceBuilder ignoreDeletes(long ignoreDeletes) {
        return super.ignoreDeletes(ignoreDeletes);
    }

    @Override
    public RowDataDeltaSourceBuilder ignoreChanges(long ignoreChanges) {
        return super.ignoreChanges(ignoreChanges);
    }

    @Override
    public RowDataDeltaSourceBuilder option(String optionName, String optionValue) {
        return super.option(optionName, optionValue);
    }

    @Override
    public RowDataDeltaSourceBuilder option(String optionName, boolean optionValue) {
        return super.option(optionName, optionValue);
    }

    @Override
    public RowDataDeltaSourceBuilder option(String optionName, int optionValue) {
        return super.option(optionName, optionValue);
    }

    @Override
    public RowDataDeltaSourceBuilder option(String optionName, long optionValue) {
        return super.option(optionName, optionValue);
    }

    @Override
    public RowDataDeltaSourceBuilder partitions(List<String> partitions) {
        return super.partitions(partitions);
    }

    @Override
    public RowDataDeltaSourceBuilder continuousMode() {
        return super.continuousMode();
    }

    @Override
    @SuppressWarnings("unchecked")
    public DeltaSource<RowData> build() {
        // TODO test this
        validateOptionExclusions();

        // TODO add option value validation. Check for null, empty values, numbers for
        //  "string" like values and string for numeric options.
        ParquetColumnarRowInputFormat<DeltaSourceSplit> format = buildFormat();

        return DeltaSource.forBulkFileFormat(tablePath, format,
            (isContinuousMode())
                ? DEFAULT_CONTINUOUS_SPLIT_ENUMERATOR_PROVIDER
                : DEFAULT_BOUNDED_SPLIT_ENUMERATOR_PROVIDER,
            hadoopConfiguration, sourceConfiguration);
    }

    private ParquetColumnarRowInputFormat<DeltaSourceSplit> buildFormat() {
        ParquetColumnarRowInputFormat<DeltaSourceSplit> format;
        if (partitions == null || partitions.isEmpty()) {
            format = buildFormatWithoutPartitions(columnNames, columnTypes, hadoopConfiguration,
                sourceConfiguration);
        } else {
            // TODO PR 8
            throw new UnsupportedOperationException("Partition support will be added later.");
            /*format =
                buildPartitionedFormat(columnNames, columnTypes, configuration, partitions,
                    sourceConfiguration);*/
        }
        return format;
    }

    // TODO PR 8
    private ParquetColumnarRowInputFormat<DeltaSourceSplit> buildPartitionedFormat(
        String[] columnNames, LogicalType[] columnTypes, Configuration configuration,
        List<String> partitionKeys, DeltaSourceConfiguration sourceConfiguration) {

        // TODO PR 8
        /*return ParquetColumnarRowInputFormat.createPartitionedFormat(
            configuration,
            RowType.of(columnTypes, columnNames),
            partitionKeys, new DeltaPartitionFieldExtractor<>(),
            sourceConfiguration.getValue(PARQUET_BATCH_SIZE),
            PARQUET_UTC_TIMESTAMP,
            PARQUET_CASE_SENSITIVE);*/
        return null;
    }

}
